# Retrieval-Augmented-Generation-RAG-evaluation


### 1) An explanation of the different types of RAG evaluation
#### Evaluating RAG Methods

###### Relevance:
- Precision & Recall: Measure how many of the retrieved pieces of information are actually useful.
- MRR (Mean Reciprocal Rank): Checks how high the first useful piece of information appears in the list.

###### Quality of the Generated Text:

- BLEU & ROUGE Scores: Compare the generated text to reference texts to see how close they are.
- Perplexity: Lower values mean the model predicts words better.

###### Combined Evaluation:

- F1 Score: A balance between precision and recall.
- Human Evaluation: Actual people judge the text for fluency, coherence, and informativeness.

###### Efficiency:
- Latency: How fast the information is retrieved.
- Throughput: How many pieces of information can be retrieved in a given time.

### 2) The type of RAG evaluation you selected, along with the reasoning behind your choice

##### Human Evaluation
- Factual Accuracy
- Coherence
- Relevance

##### Automated Metrics:
- Contextual Similarity
- BLEU/ROUGE
- BERT Score
- RAGAs Metrics

### 3) 
